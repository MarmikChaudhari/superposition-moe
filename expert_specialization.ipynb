{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893e83b1",
   "metadata": {},
   "source": [
    "# Section 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb10db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da78366a",
   "metadata": {},
   "source": [
    "### Reproducing Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367d798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_optimize(model, \n",
    "             render=False, \n",
    "             n_batch=1024,\n",
    "             steps=10_000,\n",
    "             print_freq=100,\n",
    "             lr=1e-3,\n",
    "             lr_scale=constant_lr,\n",
    "             stop_loss=0.015,\n",
    "             drop_loss=0.02,\n",
    "             hooks=[]):\n",
    "  cfg = model.config\n",
    "\n",
    "  opt = torch.optim.AdamW(list(model.parameters()), lr=lr)\n",
    "\n",
    "  start = time.time()\n",
    "  # Replace trange with regular range\n",
    "  for step in range(steps):\n",
    "    step_lr = lr * lr_scale(step, steps)\n",
    "    for group in opt.param_groups:\n",
    "      group['lr'] = step_lr\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    batch = model.generate_batch(n_batch)\n",
    "    out, load_balance_loss = model(batch)\n",
    "    error = (model.importance*(batch.abs() - out)**2)\n",
    "    reconstruction_loss = einops.reduce(error, 'b f -> f', 'mean').sum()\n",
    "    \n",
    "    loss = reconstruction_loss\n",
    "    if load_balance_loss is not None:\n",
    "      loss = loss + load_balance_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "  \n",
    "    if hooks:\n",
    "      hook_data = dict(model=model,\n",
    "                       step=step, \n",
    "                       opt=opt,\n",
    "                       error=error,\n",
    "                       loss=loss,\n",
    "                       reconstruction_loss=reconstruction_loss,\n",
    "                       load_balance_loss=load_balance_loss,\n",
    "                       lr=step_lr)\n",
    "      for h in hooks:\n",
    "        h(hook_data)\n",
    "    if step % print_freq == 0 or (step + 1 == steps):\n",
    "      print(f\"Step {step}: loss={loss.item():.6f}, lr={step_lr:.6f}\")\n",
    "    \n",
    "\n",
    "    if loss.item() < drop_loss:\n",
    "      print(f\"Dropping at step {step} with loss {loss.item():.6f}\")\n",
    "      lr = lr * 0.1\n",
    "    if loss.item() < stop_loss:\n",
    "      print(f\"Stopping at step {step} with loss {loss.item():.6f}\")\n",
    "      return loss.item()\n",
    "    \n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0eeabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(27)\n",
    "np.random.seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277007bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    n_features=2,\n",
    "    n_hidden=1,\n",
    "    n_experts=3,\n",
    "    n_active_experts=1,\n",
    "    load_balancing_loss=False,\n",
    ")\n",
    "model_base = MoEModel(config, device='cpu', feature_probability=torch.tensor(0.5), importance=torch.tensor(1))\n",
    "nn.init.xavier_normal_(model_base.gate)\n",
    "loss_base = other_optimize(model_base, n_batch=100, steps=10000, print_freq=2000, stop_loss=0.01, drop_loss=0.015)\n",
    "\n",
    "\n",
    "model_zero = MoEModel(config, device='cpu', feature_probability=torch.tensor(0.5), importance=torch.tensor(1))\n",
    "nn.init.constant_(model_zero.gate, 0)\n",
    "print(model_zero.gate)\n",
    "loss_zero = other_optimize(model_zero, n_batch=100, steps=10000, print_freq=2000, stop_loss=0.01, drop_loss=0.015)\n",
    "\n",
    "model_khot = MoEModel(config, device='cpu', feature_probability=torch.tensor(0.5), importance=torch.tensor(1))\n",
    "nn.init.constant_(model_khot.gate, 0)\n",
    "with torch.no_grad():\n",
    "    model_khot.gate.fill_diagonal_(1)\n",
    "print(model_khot.gate)\n",
    "loss_khot = other_optimize(model_khot, n_batch=100, steps=10000, print_freq=2000, stop_loss=0.01, drop_loss=0.015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122498bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_expert_specialization_multi(models_dict, resolution=100, save_path=None, losses_dict=None):\n",
    "    \"\"\"\n",
    "    Render multiple expert specialization plots with LaTeX formatting for PDF/PGF export.\n",
    "    \n",
    "    Args:\n",
    "        models_dict: Dictionary with {num_experts: model} mappings or {label: model}\n",
    "        resolution: Number of points per dimension for the grid\n",
    "        save_path: If provided, saves to {save_path}.pdf and {save_path}.pgf\n",
    "        losses_dict: Optional dictionary with {label: loss_value} to display below each plot\n",
    "    \n",
    "    Returns:\n",
    "        fig: matplotlib figure object\n",
    "    \"\"\"\n",
    "    import torch.nn.functional as F\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "    \n",
    "    # Set up matplotlib for LaTeX\n",
    "    plt.rcParams.update({\n",
    "        'text.usetex': True,\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Computer Modern'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'legend.fontsize': 12,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12\n",
    "    })\n",
    "    \n",
    "    # Create a grid of feature combinations\n",
    "    x = np.linspace(0, 1, resolution)\n",
    "    y = np.linspace(0, 1, resolution)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Flatten for batch processing\n",
    "    feature_grid = np.stack([X.flatten(), Y.flatten()], axis=1)\n",
    "    feature_tensor = torch.tensor(feature_grid, dtype=torch.float32)\n",
    "    \n",
    "    # Calculate number of subplots needed\n",
    "    n_models = len(models_dict)\n",
    "    n_cols = min(3, n_models)  # Max 3 columns\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    # Create the figure\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Define colors for experts (using tab10 colormap)\n",
    "    max_experts = max(\n",
    "        int(getattr(getattr(model, \"config\", None), \"n_experts\", model.gate.shape[0]))\n",
    "        for model in models_dict.values()\n",
    "    )\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for label, model in sorted(models_dict.items()):\n",
    "        row = plot_idx // n_cols\n",
    "        col = plot_idx % n_cols\n",
    "        ax = axes[0] if n_models == 1 else axes[row, col]\n",
    "        \n",
    "        # Determine number of experts for this model\n",
    "        num_experts = int(getattr(getattr(model, \"config\", None), \"n_experts\", model.gate.shape[0]))\n",
    "        \n",
    "        # Get expert assignments for all points\n",
    "        with torch.no_grad():\n",
    "            gate_scores = torch.einsum(\"bf,ef->be\", feature_tensor, model.gate)\n",
    "            gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "            _, top_expert_indices = torch.topk(gate_probs, k=1, dim=-1)\n",
    "            expert_assignments = top_expert_indices.squeeze(-1)\n",
    "        \n",
    "        # Reshape back to grid\n",
    "        expert_grid = expert_assignments.numpy().reshape(resolution, resolution)\n",
    "        \n",
    "        # Create discrete colormap for this model\n",
    "        colors = plt.cm.tab10.colors[:num_experts]\n",
    "        cmap = ListedColormap(colors)\n",
    "        boundaries = np.arange(-0.5, num_experts + 0.5, 1)\n",
    "        norm = BoundaryNorm(boundaries, cmap.N)\n",
    "        \n",
    "        # Create the plot\n",
    "        im = ax.imshow(expert_grid, extent=[0, 1, 0, 1], origin='lower', \n",
    "                      cmap=cmap, norm=norm, interpolation='nearest')\n",
    "        \n",
    "        #ax.set_title(f'{label}', fontsize=14, fontweight='bold', pad=15)\n",
    "        ax.set_xlabel('Feature 1', fontsize=12)\n",
    "        ax.set_ylabel('Feature 2', fontsize=12)\n",
    "        \n",
    "        # Add colorbar with expert labels\n",
    "        cbar = plt.colorbar(im, ax=ax, ticks=range(num_experts), boundaries=boundaries)\n",
    "        #cbar.set_label('Expert ID', fontsize=10)\n",
    "        cbar.set_ticklabels([f'Expert {i}' for i in range(num_experts)])\n",
    "        \n",
    "        # Add grid for better readability\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add loss text below this subplot if provided\n",
    "        if losses_dict is not None and label in losses_dict:\n",
    "            loss = losses_dict[label]\n",
    "            ax.text(0.5, -0.25, f'\\\\textbf{{Loss: {loss:.3f}}}',\n",
    "                   transform=ax.transAxes, ha='center', va='top',\n",
    "                   fontsize=16, color='black')\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Hide any unused subplots\n",
    "    for idx in range(plot_idx, n_rows * n_cols):\n",
    "        row = idx // n_cols\n",
    "        col = idx % n_cols\n",
    "        if n_rows > 1:\n",
    "            axes[row, col].set_visible(False)\n",
    "        else:\n",
    "            axes[col].set_visible(False)\n",
    "    \n",
    "    # Adjust layout with extra space at bottom if losses are provided\n",
    "    if losses_dict is not None:\n",
    "        plt.tight_layout(rect=[0, 0.02, 1, 0.98])\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    # Save if path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}.pdf\", bbox_inches='tight', dpi=300)\n",
    "        plt.savefig(f\"{save_path}.pgf\", bbox_inches='tight')\n",
    "        print(f\"Saved to {save_path}.pdf and {save_path}.pgf\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"1\": loss_zero, \"2\": loss_base, \"3\": loss_khot}\n",
    "render_expert_specialization_multi({\"1\": model_zero, \"2\": model_base, \"3\": model_khot}, losses_dict=losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00291343",
   "metadata": {},
   "source": [
    "### Reproducing Figure 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a8bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "np.random.seed(41)\n",
    "torch.manual_seed(41)\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_fig6 = Config(\n",
    "    n_features = 20,\n",
    "    n_hidden = 5,\n",
    "    n_experts = 4,\n",
    "    n_active_experts = 1,\n",
    "    load_balancing_loss = False,\n",
    ")\n",
    "\n",
    "model_6a = MoEModel(\n",
    "    config=config_fig6,\n",
    "    device=DEVICE,\n",
    "    importance = 0.7**torch.from_numpy(np.arange(config_fig6.n_features)),\n",
    "    feature_probability = torch.tensor(0.09)\n",
    ")\n",
    "\n",
    "model_6b = MoEModel(\n",
    "    config=config_fig6,\n",
    "    device=DEVICE,\n",
    "    importance = 0.7**torch.from_numpy(np.arange(config_fig6.n_features)),\n",
    "    feature_probability = torch.tensor(0.1)\n",
    ")\n",
    "\n",
    "model_6c = MoEModel(\n",
    "    config=config_fig6,\n",
    "    device=DEVICE,\n",
    "    importance = 0.7**torch.from_numpy(np.random.choice(config_fig6.n_features, config_fig6.n_features, replace=False)),\n",
    "    feature_probability = torch.tensor(0.1)\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize gate matrix to the diagonal (for 6a)\n",
    "nn.init.constant_(model_6a.gate, 0)\n",
    "with torch.no_grad():\n",
    "    model_6a.gate.fill_diagonal_(1)\n",
    "\n",
    "# Initialize gate matrix to increments of 5 (for 6b)\n",
    "indices = np.arange(model_6b.gate.shape[1]) # NOT RANDOM ON THIS ONE\n",
    "indices = torch.from_numpy(indices.reshape(model_6b.gate.shape[0], -1))\n",
    "print(indices)\n",
    "print(model_6b.importance)\n",
    "nn.init.constant_(model_6b.gate, 0)\n",
    "with torch.no_grad():\n",
    "    for i in range(model_6b.gate.shape[0]):\n",
    "        model_6c.gate[i, indices[i]] = 1\n",
    "\n",
    "# Random k-hot initialize the gate matrix (for 6c)\n",
    "indices = np.random.choice(model_6c.gate.shape[1], size=int(model_6c.gate.shape[0]*5), replace=False)\n",
    "indices = torch.from_numpy(indices.reshape(model_6c.gate.shape[0], -1))\n",
    "nn.init.constant_(model_6c.gate, 0)\n",
    "with torch.no_grad():\n",
    "    for i in range(model_6c.gate.shape[0]):\n",
    "        model_6c.gate[i, indices[i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training first model...\")\n",
    "optimize(model_6a, n_batch=1024, steps=10000, print_freq=1000)\n",
    "print(f\"Training second model...\")\n",
    "optimize(model_6b, n_batch=1024, steps=10000, print_freq=1000)\n",
    "print(f\"Training last model...\")\n",
    "optimize(model_6c, n_batch=1024, steps=10000, print_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30641ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_features_bar(model, save_path=None):\n",
    "    \"\"\"Render bar plots showing feature norms colored by polysemanticity\"\"\"\n",
    "    cfg = model.config\n",
    "    # expert weights\n",
    "    W_exp = model.W_experts.detach()\n",
    "    W_norm = W_exp / (1e-5 + torch.linalg.norm(W_exp, 2, dim=2, keepdim=True))\n",
    "\n",
    "    interference = torch.einsum('ifh,igh->ifg', W_norm, W_exp) # (n_experts, n_features, n_features)\n",
    "    interference[:, torch.arange(cfg.n_features), torch.arange(cfg.n_features)] = 0 # set diagonal to 0\n",
    "\n",
    "    polysemanticity = torch.linalg.norm(interference, dim=-1).cpu()\n",
    "    norms = torch.linalg.norm(W_exp, 2, dim=-1).cpu()\n",
    "\n",
    "    x = torch.arange(cfg.n_features)\n",
    "    \n",
    "    # set up matplotlib for LaTeX\n",
    "    plt.rcParams.update({\n",
    "        'text.usetex': True,\n",
    "        'font.family': 'serif',\n",
    "        'font.serif': ['Computer Modern'],\n",
    "        'font.size': 12,\n",
    "        'axes.labelsize': 14,\n",
    "        'axes.titlesize': 16,\n",
    "        'legend.fontsize': 12,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12\n",
    "    })\n",
    "    \n",
    "    fig, axes = plt.subplots(cfg.n_experts, 1, figsize=(6, 4 * cfg.n_experts))\n",
    "    if cfg.n_experts == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for expert_idx in range(cfg.n_experts):\n",
    "        ax = axes[expert_idx]\n",
    "        \n",
    "        # Create bar plot with color mapping and narrower bars\n",
    "        bars = ax.bar(x, norms[expert_idx], \n",
    "                     color=plt.cm.viridis(polysemanticity[expert_idx] / polysemanticity[expert_idx].max()),\n",
    "                     width=0.6)\n",
    "        \n",
    "        # Add colorbar using ScalarMappable\n",
    "        sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=0, vmax=polysemanticity[expert_idx].max()))\n",
    "        sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax)\n",
    "        cbar.set_label('Superposition')\n",
    "        \n",
    "        # Add vertical line at n_hidden boundary\n",
    "        ax.axvline(x=(cfg.n_hidden-0.5), color='red', linestyle='--', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        ax.set_title(f'Expert {expert_idx}')\n",
    "        #ax.set_xlabel('Features $\\\\rightarrow$')\n",
    "        #ax.set_ylabel('Norm $||W_i||$')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Set x-axis ticks to go from 0 to 5 to 10 to 15\n",
    "        ax.set_xticks([0, 5, 10, 15])\n",
    "        \n",
    "        # Set x-axis limits to reduce horizontal width\n",
    "        ax.set_xlim(-0.5, cfg.n_features - 0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(f\"{save_path}_bar.pdf\", bbox_inches='tight')\n",
    "        plt.savefig(f\"{save_path}_bar.pgf\", bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    #return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_features_bar(model_6a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_features_bar(model_6b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c1788",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_features_bar(model_6c)\n",
    "print(indices)\n",
    "print(model_6c.importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761d909",
   "metadata": {},
   "source": [
    "### Reproducing Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601fb5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.model import optimize_vectorized\n",
    "from helpers.helpers import my_gate_init, save_initial_weights\n",
    "from functools import partial\n",
    "import os\n",
    "cfg = Config(\n",
    "    n_features = 100,\n",
    "    n_hidden = 10,\n",
    "    n_experts = 10,\n",
    "    n_active_experts = 1,\n",
    "    load_balancing_loss = False,\n",
    ")\n",
    "N = 200\n",
    "DEVICE = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "feature_prob = torch.tensor(0.1).to(DEVICE)\n",
    "importance = 0.9**torch.from_numpy(np.random.choice(cfg.n_features, cfg.n_features, replace=False)).to(DEVICE)\n",
    "\n",
    "feature_probs = [feature_prob for _ in range(N)]\n",
    "configs = [cfg for _ in range(N)]\n",
    "importances = [importance for _ in range(N)]\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# 2) Train vectorized\n",
    "final_losses, stacked_params = optimize_vectorized(\n",
    "    configs,\n",
    "    feature_probs,\n",
    "    importances,\n",
    "    device=DEVICE,\n",
    "    n_batch=1024,\n",
    "    steps=10000,\n",
    "    lr=1e-3,\n",
    "    print_freq=1000,\n",
    "    init_fn=my_gate_init,\n",
    "    on_initialized=partial(save_initial_weights, save_path=\"models/hundred_feats_vectorized_init\")\n",
    ")\n",
    "print(\"Per-model losses:\", final_losses.tolist())\n",
    "\n",
    "print(stacked_params.keys())\n",
    "gate = stacked_params[\"gate\"]\n",
    "print(gate.shape)\n",
    "snapshot = {\n",
    "    \"configs\": configs,\n",
    "    \"feature_probs\": [fp if torch.is_tensor(fp) else torch.tensor(fp) for fp in feature_probs],\n",
    "    \"importances\": [imp.detach().cpu() for imp in importances],\n",
    "    \"stacked_params\": {k: v.detach().cpu().clone() for k, v in stacked_params.items()},\n",
    "}\n",
    "torch.save(snapshot, \"models/post_training_vectorized_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_models = torch.load(\"models/post_training_vectorized_weights.pt\", weights_only=False)\n",
    "stacked_params = vectorized_models['stacked_params']\n",
    "W_exp = stacked_params['W_experts']\n",
    "W_norm = W_exp / (1e-5 + torch.linalg.norm(W_exp, 2, dim=3, keepdim=True))\n",
    "\n",
    "interference = torch.einsum('...ifh,...igh->...ifg', W_norm, W_exp) # (n_experts, n_features, n_features)\n",
    "interference[:, :, torch.arange(100), torch.arange(100)] = 0 # set diagonal to 0\n",
    "polysemanticity = torch.linalg.norm(interference, dim=-1).cpu()\n",
    "\n",
    "mask = polysemanticity < 0.2\n",
    "polysemanticity[~mask] = float('inf')\n",
    "#new_poly = polysemanticity.flatten(start_dim=0, end_dim=1)\n",
    "counts = mask.sum(dim=2)#.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "xavier_mono_feats = []\n",
    "khot_mono_feats = []\n",
    "for i in range(100):\n",
    "    expert_list = []\n",
    "    for j in range(10):\n",
    "        expert_feat_count = counts[i][j]\n",
    "        if expert_feat_count > 0:\n",
    "            expert_list.append((j, polysemanticity[i][j].topk(expert_feat_count, largest=False)[1]))\n",
    "    print(expert_list)\n",
    "    xavier_mono_feats.append(expert_list)\n",
    "print(\"Moving from xavier init to khot init\")\n",
    "for i in range(100, 200):\n",
    "    expert_list = []\n",
    "    for j in range(10):\n",
    "        expert_feat_count = counts[i][j]\n",
    "        if expert_feat_count > 0:\n",
    "            expert_list.append((j, polysemanticity[i][j].topk(expert_feat_count, largest=False)[1]))\n",
    "    print(expert_list)\n",
    "    khot_mono_feats.append(expert_list)   \n",
    "\n",
    "print(xavier_mono_feats)\n",
    "print(khot_mono_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gen_func = MoEModel.generate_batch\n",
    "\n",
    "def dataset_conditional_batch_generation(model, info, n_batch=1024, clamp_value: Optional[float] = None):\n",
    "    expert_id, feature_indices = info\n",
    "    \"\"\"Fix certain features and sample the other features according to config statistics\"\"\"\n",
    "    # print(f\"Using statistics from model. Feature probability: {model.feature_probability}\")\n",
    "    print(f'Expert {expert_id} monosemantically represents features: {feature_indices}')\n",
    "    batch = batch_gen_func(model, n_batch)\n",
    "    if clamp_value is not None:\n",
    "        batch[:, feature_indices] = clamp_value\n",
    "    else:\n",
    "        # Sample uniformly between 0.5 and 1\n",
    "        clamps = torch.rand(batch.shape[0], 1) #* 0.5 + 0.5\n",
    "        batch[:, feature_indices] = clamps\n",
    "    #print(batch)\n",
    "    return batch\n",
    "    # print(batch.shape)\n",
    "\n",
    "def batch_only_feature_active_gen(model, info, n_batch=1024, clamp_value: Optional[float] = None):\n",
    "    expert_id, feature_indices = info\n",
    "    \"\"\"Fix certain features and sample the other features according to config statistics\"\"\"\n",
    "    # print(f\"Using statistics from model. Feature probability: {model.feature_probability}\")\n",
    "    print(f'Expert {expert_id} monosemantically represents features: {feature_indices}')\n",
    "    batch = torch.zeros(n_batch, model.config.n_features)\n",
    "    clamps = torch.rand(batch.shape[0], 1)\n",
    "    batch[:, feature_indices] = clamps\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COLUMN_CHOICE = \"unchanged\"\n",
    "COLUMN_CHOICE = \"active (clamped to 1)\"\n",
    "#COLUMN_CHOICE = \"only feature active (all other features=0)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211439f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gates = stacked_params['gate']\n",
    "xav_mean_usage_counts = {1: [], 2: [], 3: [], 4: [],}\n",
    "khot_mean_usage_counts = {1: [], 2: [], 3: [], 4: [], 5: [],}\n",
    "xav_medians = []\n",
    "khot_medians = []\n",
    "model = MoEModel(config=vectorized_models['configs'][0], device=\"cpu\") # simply used for generating the input batch\n",
    "\n",
    "batch_gen_functions = {\n",
    "    \"unchanged\": batch_gen_func,\n",
    "    \"active (clamped to 1)\": dataset_conditional_batch_generation,\n",
    "    \"only feature active (all other features=0)\": batch_only_feature_active_gen\n",
    "}\n",
    "\n",
    "for i in range(len(xavier_mono_feats)):\n",
    "    for j in range(len(xavier_mono_feats[i])):\n",
    "        gate = gates[i]\n",
    "        func = batch_gen_functions[COLUMN_CHOICE]\n",
    "        if COLUMN_CHOICE == \"unchanged\":\n",
    "            batch = func(model, n_batch=8192)\n",
    "        else:\n",
    "            batch = func(model, xavier_mono_feats[i][j], n_batch=8192, clamp_value=1.0)\n",
    "        gate_scores = torch.einsum(\"...f,ef->...e\", batch, gate)\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "        gate_probs_topk = gate_probs.topk(1, dim=-1)\n",
    "\n",
    "        values, indices = gate_probs_topk\n",
    "        counts = indices.flatten().bincount()\n",
    "        if counts.shape[0] < model.config.n_experts:\n",
    "            counts = torch.cat([counts, torch.zeros(model.config.n_experts - counts.shape[0])])\n",
    "\n",
    "        counts = counts / counts.sum()\n",
    "\n",
    "        xav_medians.append(torch.median(counts).item())\n",
    "\n",
    "        num_feats = xavier_mono_feats[i][j][1].shape[0]\n",
    "        exp_num = xavier_mono_feats[i][j][0]\n",
    "        xav_mean_usage_counts[num_feats].append(counts[exp_num].detach().item())\n",
    "\n",
    "print(\"ENDING XAV, STARTING KHOT\")\n",
    "\n",
    "for i in range(len(khot_mono_feats)):\n",
    "    for j in range(len(khot_mono_feats[i])):\n",
    "        gate = gates[i+100]\n",
    "        func = batch_gen_functions[COLUMN_CHOICE]\n",
    "        if COLUMN_CHOICE == \"unchanged\":\n",
    "            batch = func(model, n_batch=8192)\n",
    "        else:\n",
    "            batch = func(model, khot_mono_feats[i][j], n_batch=8192, clamp_value=1.0)\n",
    "        gate_scores = torch.einsum(\"...f,ef->...e\", batch, gate)\n",
    "        gate_probs = F.softmax(gate_scores, dim=-1)\n",
    "        gate_probs_topk = gate_probs.topk(1, dim=-1)\n",
    "\n",
    "        values, indices = gate_probs_topk\n",
    "        counts = indices.flatten().bincount()\n",
    "        if counts.shape[0] < model.config.n_experts:\n",
    "            counts = torch.cat([counts, torch.zeros(model.config.n_experts - counts.shape[0])])\n",
    "\n",
    "        counts = counts / counts.sum()\n",
    "\n",
    "\n",
    "        khot_medians.append(torch.median(counts).item())\n",
    "\n",
    "        num_feats = khot_mono_feats[i][j][1].shape[0]\n",
    "        exp_num = khot_mono_feats[i][j][0]\n",
    "        khot_mean_usage_counts[num_feats].append(counts[exp_num].detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642c748",
   "metadata": {},
   "outputs": [],
   "source": [
    "gates = stacked_params['gate']\n",
    "\n",
    "print(\"Xavier Initialization Scheme\\n\")\n",
    "print(f\"Number of experts with 1 monosemantic feature: {len(xav_mean_usage_counts[1])}\")\n",
    "print(f\"Number of experts with 2 monosemantic feature: {len(xav_mean_usage_counts[2])}\")\n",
    "print(f\"Number of experts with 3 monosemantic feature: {len(xav_mean_usage_counts[3])}\")\n",
    "print(f\"Number of experts with 4 monosemantic feature: {len(xav_mean_usage_counts[4])}\\n\")\n",
    "\n",
    "mean_usage_one = sum(xav_mean_usage_counts[1]) / len(xav_mean_usage_counts[1])\n",
    "mean_usage_two = sum(xav_mean_usage_counts[2]) / len(xav_mean_usage_counts[2])\n",
    "mean_usage_three = sum(xav_mean_usage_counts[3]) / len(xav_mean_usage_counts[3])\n",
    "mean_usage_four = sum(xav_mean_usage_counts[4]) / len(xav_mean_usage_counts[4])\n",
    "#print(f\"Average usage of experts with 1,2,3,4 monosemantic features, when that feature is {COLUMN_CHOICE}:\")\n",
    "print(f\"\\nAverage usage of experts w/ 1 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_one}\")\n",
    "print(f\"Average usage of experts w/ 2 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_two}\")\n",
    "print(f\"Average usage of experts w/ 3 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_three}\")\n",
    "print(f\"Average usage of experts w/ 4 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_four}\")\n",
    "#print(mean_usage_one, mean_usage_two, mean_usage_three, mean_usage_four)\n",
    "\n",
    "xav_mean_med = sum(xav_medians) / len(xav_medians)\n",
    "xav_mean_std = torch.std(torch.as_tensor(xav_medians)).item()\n",
    "#print(xav_mean_med, xav_mean_std)\n",
    "\n",
    "print(\"\\nK-Hot Initialization Scheme\\n\")\n",
    "print(f\"Number of experts with 1 monosemantic feature: {len(khot_mean_usage_counts[1])}\")\n",
    "print(f\"Number of experts with 2 monosemantic feature: {len(khot_mean_usage_counts[2])}\")\n",
    "print(f\"Number of experts with 3 monosemantic feature: {len(khot_mean_usage_counts[3])}\")\n",
    "print(f\"Number of experts with 4 monosemantic feature: {len(khot_mean_usage_counts[4])}\")\n",
    "mean_usage_one = sum(khot_mean_usage_counts[1]) / len(khot_mean_usage_counts[1])\n",
    "mean_usage_two = sum(khot_mean_usage_counts[2]) / len(khot_mean_usage_counts[2])\n",
    "mean_usage_three = sum(khot_mean_usage_counts[3]) / len(khot_mean_usage_counts[3])\n",
    "mean_usage_four = sum(khot_mean_usage_counts[4]) / len(khot_mean_usage_counts[4])\n",
    "mean_usage_five = sum(khot_mean_usage_counts[5]) / len(khot_mean_usage_counts[5]) if len(khot_mean_usage_counts[5]) else 0.0\n",
    "#print(f\"Average usage of experts with 1,2,3,4 monosemantic features, when that feature is {COLUMN_CHOICE}:\")\n",
    "print(f\"\\nAverage usage of experts w/ 1 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_one}\")\n",
    "print(f\"Average usage of experts w/ 2 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_two}\")\n",
    "print(f\"Average usage of experts w/ 3 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_three}\")\n",
    "print(f\"Average usage of experts w/ 4 monosemantic features, when that feature(s) is {COLUMN_CHOICE}:\\n {mean_usage_four}\")\n",
    "#print(mean_usage_one, mean_usage_two, mean_usage_three, mean_usage_four, mean_usage_five)\n",
    "\n",
    "khot_mean_med = sum(khot_medians) / len(khot_medians)\n",
    "khot_mean_std = torch.std(torch.as_tensor(khot_medians)).item()\n",
    "#print(khot_mean_med, khot_mean_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
